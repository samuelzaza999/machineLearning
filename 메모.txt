(지능, intelligence , 知能)
현상을 이해하고 대처 방법을 알아내는
지적 활동 능력

AI (Artificial Intelligence),
그러한 지능을 사람이
컴퓨터에 구현한 것

4차 산업 혁명:
SNS, 빅데이터, 사물인터넷,
클라우드 컴퓨팅, 가상현실(VR),
증강현실(AR), 3D 프린팅, 로봇,
자율주행차, 드론, 바이오·나노
기술, 소재과학, 유전자가위,
양자컴퓨터, 블록체인, 인공지능
등의 첨단기술들이 발전·융합
범용 플랫폼, 모든 곳에 활용가능
제품과 서비스의 지능화(AI)

도전의 중요성 : 두려움?

흐름

------------------------------
https://www.kaggle.com/yungbyun

머신러닝 : 기계의 알고리즘, 데이터로 구현
딥러닝 : 뇌의 알고리즘

-----------
캐글 실습 : 깃허브 - 성별 인식 코드
세션 켜고 Run All


머신러닝 ::
구구단처럼 문제와 정답을 학습, 중간에 잘라 학습과 테스트 데이터 분리
-> 학습은 다 확인하고 테스트는 풀게 해 정답과 대조
ex) 키, 무게, 발크기, 학년 -> 성별

aaa = svm.SVC()
aaa.fit('학습용 문제', '정답')
prediction = aaa.predict('테스트용 문제')

어느 방식이든 객체가 받는 클래스만 바뀌고 적용하는 방법은 위와 동일.

서포트 벡터 머신(SVC) : 분류에서 결정적 도움 주는 인접 자료.

데이터 읽어오기
-> 시각화하기
-> 학습용/테스트용 나누기
-> 머신러닝 모델 학습시키기
-> 테스트하기(예측, 분류)

166,57,240,1, 0
178,92,265,1, 1
167,80,270,1, 1
168,52,245,2, 1
155,60,235,2, 0
163,45,230,2, 0
160,53,235,3, 0
180,77,260,4, 1
167,71,260,2, 1
160,51,245,2, 0
162,53,240,2, 0
180,82,280,6, 1
172,90,255,6, 1
160,51,245,5, 0

155,66,245,5, 0
163,54,242,5, 0
177,88,263,5, 1
166,82,268,6, 1
170,53,247,6, 1
154,59,234,1, 0
164,47,232,1, 0

인공지능 : 별도 데이터 없이 구현 가능
머신러닝 : 데이터를 주고 학습

해쉬 함수 :
한 방향으로 데이터 변조하면 복구 불가. 블록체인 -> 데이터와 해쉬 비교해 변조 확인
해쉬 작업의 대가로 랜덤하게 코인 생성(채굴)

엔진 이전에 운전부터.
모듈화 자주하고 파라미터만 돌려막기로 능률 향상

파이썬 '필드 column' 줘서 모듈화한 식에 때려박기 -> 일반적인 사용 가능

---------------- 0322
데이터 읽어오기
-> 표시하기
-> 학습용/테스트용 나누기
-> 머신러닝 모델 학습시키기
-> 테스트하기(예측, 분류)
 = 읽 표 나 학 테

gil = DecisionTreeClassifier()
gil.fit('학습용 문제', '정답')
prediction = gil.predict('테스트용 문제')


>>> Female/Male Classification_ML_Simple

읽기 >
csv 파일 읽기 :
import pandas as pd
data = pd.read_csv("asdf.csv")

표시 >
점 찍기 :
plot(data, "Height", "Weight", "Sex")
바이올린 형태 분포도 :
violinplot(data, "Height", "Sex")

나누기 >
학습용 80%, 테스트용 20%로 데이터 나누기 :
train, test = split(data, 0.8)
학습 입력 & 정답 :
train_x = train[['Height', 'FeetSize', 'Weight']]
train_y = train.Sex
테스트 입력 & 정답 :
test_x = test[['Height', 'FeetSize', 'Weight']]
test_y = test.Sex

---------------- 0323
학습 >
정답 5개 호출 :
test_x.head(5)
학습시키기 & 예측하기 :
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
gil = svm.SVC()
gil.fit(train_x, train_y)
prediction = gil.predict(test_x)

테스트 >
print('인식률 : ', metrics.accuracy_score(prediction, test_y) * 100)
gil = LogisticRegression()
gil = KNeighborsClassifier(n_neighbors = 3)


>>> plant_diary_original_simple

데이터 처리 모듈 :
import pandas as pd
테이터 시각화 모듈 :
import matplotlib.pyplot as plt
import seaborn as sns
데이터 분할 모듈 :
from sklearn.model_selection import train_test_split

from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor

나누기 >
train, test = split(data, 0.8)
train_x = train[['day']]
train_y = train.height
test_x = test[['day']]
test_y = test.height

학습(4가지) >
취사선택 :
gil = LinearRegression()
gil = KNeighborsRegressor(n_neighbors = 2)
gil = RandomForestRegressor(n_estimators = 28, random_state = 0)
gil = DecisionTreeRegressor(random_state = 0)LinearRegression()
이후 공통 사항 :
gil.fit(train_x, train_y)
score = gil.score(test_x, test_y)
print('score : ', format(score, '.3f'))

테스트 >
prediction = gil.predict(test_x)
print('예상 : ', prediction)
print('정답 : ', test_y)


★머신러닝의 핵심 ::
종류 이거 or 저거? : 분류
-> 비가 올까 안 올까, 더울까 추울까, 학년이 어떻게 될까
값이 얼만큼 될까? : 예측
-> 몸무게가 어떻게 될까, 내일 온도는 어떨까


머신러닝 중에서 사람의 뇌를 모방 : 딥러닝.

-------------- 0329
딥러닝 :: 시냅스(연결부위, 신경망)의 구현.
★시냅스에서 신경전달물질(단백질)이 수용체로 이동, 물질이 많이 나올 수록 전달되는 전기신호가 강해진다.
뇌는 신호가 발생 or X로만 구분.
신호 * 신경전달물질의 양 = 전기신호 -> 특정 레벨 이상일 때만 발생 판정 -> 신경세포 핵에서 전기신호 합치기
신경전달물질의 설정에 따라 전기신호가 다르게 발생. 걷는 용도, 기억하는 용도 등

---------------- 0330
신호 -> 신경전달물질 -> 물질이 수용체로 이동 -> 반대편 뉴런 자극 -> 신경전달물질을 원래 자리 회수 -> 반복

뇌 문제 :
시냅스 작동 X -> 마비, 치매 등
= 시냅스 설정 X, 신경전달물질의 양 제대로 설정 X
-> 딥러닝은 신경전달물질의 양을 바꾸는 프로그래밍.

습관 바꾸기? :
부정적 이미지 심기. 고통 주입. -> 신경전달물질의 양 변화로 기억하기.
= 뇌내 학습의 원리? : 행복은 늘리고 고통은 줄이는 방향으로.

그림책 : 입력을 주고 정답을 알려준다. -> 학습으로 신경전달물질 설정하기.
gil.fit(<그림>, '오리')


뉴런의 동작(출력) ::
신경세포가 단 하나라 가정. (w=1)
신경세포 출력 h = 입력 x * 연결강도(신경전달물질 양 : 단절/약/강) w
w=1인 뉴런 : h = 1x

★학습이란? :
신경전달물질의 양(w)을 조절하는 것.

ex)
1시간(x) 공부하면 1시간(ground truth) 게임.
그럼 4시간 공부에 몇 시간 게임할 수 있을지(prediction) w값 구하기.
h = w * x(=1)
입력과 정답이 정해졌다면 강도의 변화에 따라 오차를 찾아내(야단) 줄여나간다.

실제로는 여러 개의 입력. 입력의 수만큼 연결도 존재.
-> 각 입력마다 시냅스의 가중치를 곱한 뒤 모두 더한다. (weighted sum)

뉴런은 weighted sum이 일정 값 이상일 때만 신호 발생. (계단 함수)
= 특정 값(T) 이상이면 ON(1), 아니면 OFF(0)
신경세포 핵에서 weighted sum & 계단 함수 역할.

★시험 : 뉴런 그리기.
h = 1x
h = x1 + 2x2 + x3 + 3x4
h = 1 (if x1 + 2x2 + x3 + 3x4 > T) / 0 (otherwise)

-------------- 0405
아기가 불을 만져도 떼지 못하는 이유?
: 시냅스가 설정되지 않아서 뉴런 간에 고통 -> 회피 구성이 없음.
뉴런의 출력 = 대답.


[선형회귀]
회귀(regression) :
자연의 법칙, 현상. 그에 따른 예측이 가능.
ex) 연어, 연령과 힘의 관계

선형회귀 :
비례 관계의 회귀. 선형회귀를 모아서 비선형 예측 가능.
-> 회귀 형태가 비선형이어도 선형회귀라 할 수 있다.
ex) 집의 크기-가격의 직선 그래프
x가 어떠한 선형결합 상태인지.
y축(가격) : 뉴런이 맞춰야 할 정답, 그를 결정하는 팩터들이 많다. 학군, 혐오시설...

---------------- 0406
https://www.desmos.com/	그래프 그리기.
csv : 컴마로 구분된, 학습할 데이터가 있는 파일

ex) 연결강도 w 찾기.
인스턴스 (1,1),(2,2),(3,3) 보고 h=1x 유추해내면 정확.
만약 w=2? -> 오류. -> 수정 요청하면 학습, 맞을 때까지

경험이란? : 다양한 경우를 csv로 저장한다.
입력이 2개면 3차원 그래프. 그 이상은 X

h = wx + b
하나의 뉴런은 하나의 선형회귀를 표현. 직선이 회귀.
h : 가설, 뉴런의 대답
연결강도 w, 바이어스 b : 시냅스 연결강도의 값. 다양한 회귀 표현 가능.
-> ★w와 b를 어떻게 조절? : 뉴런이 정답을 더 잘 대답하도록 조절하기.

★★그래프 모양의 유래 설명하기.
뉴런이 맞는 대답을 하는지 아는 방법? :
h와 기존의 정답(ground truth) y의 오차를 계산.
★차이는 간격이므로 음수가 나지 않는다. -> 절대값.
-> ★오류 E = |xw - y| (x축: w, y축: E)
-> ★학습이란? : 오류(E)가 거의 없도록 w를 조절하는 것.

가설(h) : 어떤 현상을 설명, 표현한 것. 증명되지 않았지만 실험과 조절로 검증할 수 있는 것.

ex)
E = |w*1 - 1| 학습 데이터? -> 1,1
csv (1,1)(2,2)(3,3) 오류값? -> E = |w*1 - 1| + |w*2 - 2| + |w*3 - 3|
E = |w*1 - 0.5| 학습 완료? -> w=0.5
E = |w*1 - 3| 학습 완료 그래프? -> h=3x

지도학습 : 입력과 정답을 알려주면서 학습한다.


위에 나온 절대값 오류 함수는 V자 형태, 오류 줄도록 w 조절하려면?
-> 경사하강. : 기울기를 이용해 계속 경사 아래로 내려가면 에러가 없는 지점에 도달할 것.

다음 w값 좌표 : w1 = w0 - a*기울기 (a : 반영 비율)
ex) w = 4 - 1*1 or w = -2 - 1*(-1)

------------- 0412
yungcheolbyun@gmail.com
절댓값 오류 함수에서 밖으로 튕겨나갈 때의 용어? 학습률 오버슈팅.
오늘 내로.

절대값 오류 함수의 기울기로 w를 고친다.
★경사하강 수식 :
w = w - a*기울기 (★기울기는 w지점에서의 기울기.)
ex) E = |w - 1|
w=4 : w = 4 - 1*1 -> w=3
-> w=3 : w = 3 - 1*1 -> w=2
-> w=2 : w = 2 - 1*1 -> w=1

w=-2 : w = -2 - 1*(-1) -> w=-1 ~

if a(반영 비율) = 2?
w=-2 : w = -2 - 2*(-1) -> w=0
-> w=0 : w = 0 - 2*(-1) -> w=2
-> w=2 : w = 2 - 2*(1) -> w=0 무한 루프

절대값 오류? :
1. 왼쪽-오른쪽은 항상 같은 기울기 = 항상 같은 경사하강 속도.
2. w값 조절 실패 가능
3. w=1일 때 기울기 구할 수 없다.
-> 기울기만 보고 w값 짐작 불가.


제곱 오류 함수 (square error func) :
E = |xw - y|^2
E=0에 가까울 수록 에러는 더 작아진다.
ex) 0.5*1=0.5, 한편 0.5*0.5=0.25

마찬가지로 w 지점의 기울기만 구한다면 경사하강 가능.
-> ★미분 : x축 변화를 아주 작게 해 접선으로 정확한 기울기 구하기.
lim(∆w->0)∆E/∆w = ∂E/∂w
w를 아주 조금 늘렸을 때(∂w) E는 얼마나 늘어날까.(∂E)
델타 = 차이

sin을 미분하면 cos

------------ 0413
기울기 구하는 표현 : lim(∆w->0)∆E/∆w = ∂E/∂w
미분 없이도 가능, 빼는 차이가 작아야.
★개같은 그래프 주고 미분하라고 할 가능성. 기울기 0 부분 확인. 증감 속도 고려.

★에러 그래프에 제곱을 한 이유? :
에러 최저점을 찾을 때, 멀리 있는 값의 기울기는 커지고 가까운 값의 기울기는 급격히 작아져 최저점에 근접하기 쉬워진다.
+ 모든 지점에서 기울기 계산 가능.
w 지점의 기울기가 매우 크다? : 최저점에서 멀리 있다.
반대로 매우 작다? : 최저점 근처에 있다.
음식의 간 맞추기, 주차, ~

지도학습 : 입력과 정답을 알려주면서 학습한다.

평균 제곱 오류 함수(MSE) : 오차를 나타내는 척도
E = 1/3 * ((w*1 - 1)^2 + (w*2 - 2)^2 + (w*3 - 3)^2)
-> E = 1/N * N∑{i=1}(wxi - yi)^2 [N개 데이터]

https://gooopy.tistory.com/68
★에폭(epoch) :
모든 데이터를 한 번씩 학습한 횟수, 너무 많으면 오버피팅.
★배치 사이즈(batch size) :
연산 한 번에 들어가는 데이터 크기.

제곱 함수 등으로 인해 학습량 너무 많을 때 : 크게 묶어 정규화한 값들 여러 개로 판단
-> 저 둘을 적당히 조절해야 최적의 속도로 구할 수 있다.

기울기가 크다?(∆E/∆w) :
w 조절하면 E가 크게 변화 -> w 변화가 E에 미치는 영향 크다.
기울기가 작다? :
w 조절하면 E가 작게 변화 -> w 변화가 E에 미치는 영향 작다.

ex) 미치는 영향 구하기.
E = (wx - y)^2에서 데이터 (1,1) 주어질 때, w=3 지점에서 w 변화가 E에 미치는 영향?
1) 작은 차이 대입 2) w 기준 미분

https://github.com/yungbyun/myml/blob/master/n01_simple.py
구글 코랩 실행, plt 추가
★코드 fit 역할 : 에폭 횟수만큼 학습시키기.

=중간 : 26일 오후 3시쯤=