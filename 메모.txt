(지능, intelligence , 知能)
현상을 이해하고 대처 방법을 알아내는
지적 활동 능력

AI (Artificial Intelligence),
그러한 지능을 사람이
컴퓨터에 구현한 것

4차 산업 혁명:
SNS, 빅데이터, 사물인터넷,
클라우드 컴퓨팅, 가상현실(VR),
증강현실(AR), 3D 프린팅, 로봇,
자율주행차, 드론, 바이오·나노
기술, 소재과학, 유전자가위,
양자컴퓨터, 블록체인, 인공지능
등의 첨단기술들이 발전·융합
범용 플랫폼, 모든 곳에 활용가능
제품과 서비스의 지능화(AI)

도전의 중요성 : 두려움?

흐름

------------------------------
https://www.kaggle.com/yungbyun

머신러닝 : 기계의 알고리즘, 데이터로 구현
딥러닝 : 뇌의 알고리즘

-----------
캐글 실습 : 깃허브 - 성별 인식 코드
세션 켜고 Run All


머신러닝 ::
구구단처럼 문제와 정답을 학습, 중간에 잘라 학습과 테스트 데이터 분리
-> 학습은 다 확인하고 테스트는 풀게 해 정답과 대조
ex) 키, 무게, 발크기, 학년 -> 성별

aaa = svm.SVC()
aaa.fit('학습용 문제', '정답')
prediction = aaa.predict('테스트용 문제')

어느 방식이든 객체가 받는 클래스만 바뀌고 적용하는 방법은 위와 동일.

서포트 벡터 머신(SVC) : 분류에서 결정적 도움 주는 인접 자료.

데이터 읽어오기
-> 시각화하기
-> 학습용/테스트용 나누기
-> 머신러닝 모델 학습시키기
-> 테스트하기(예측, 분류)

166,57,240,1, 0
178,92,265,1, 1
167,80,270,1, 1
168,52,245,2, 1
155,60,235,2, 0
163,45,230,2, 0
160,53,235,3, 0
180,77,260,4, 1
167,71,260,2, 1
160,51,245,2, 0
162,53,240,2, 0
180,82,280,6, 1
172,90,255,6, 1
160,51,245,5, 0

155,66,245,5, 0
163,54,242,5, 0
177,88,263,5, 1
166,82,268,6, 1
170,53,247,6, 1
154,59,234,1, 0
164,47,232,1, 0

인공지능 : 별도 데이터 없이 구현 가능
머신러닝 : 데이터를 주고 학습

해쉬 함수 :
한 방향으로 데이터 변조하면 복구 불가. 블록체인 -> 데이터와 해쉬 비교해 변조 확인
해쉬 작업의 대가로 랜덤하게 코인 생성(채굴)

엔진 이전에 운전부터.
모듈화 자주하고 파라미터만 돌려막기로 능률 향상

파이썬 '필드 column' 줘서 모듈화한 식에 때려박기 -> 일반적인 사용 가능

---------------- 0322
데이터 읽어오기
-> 표시하기
-> 학습용/테스트용 나누기
-> 머신러닝 모델 학습시키기
-> 테스트하기(예측, 분류)
 = 읽 표 나 학 테

gil = DecisionTreeClassifier()
gil.fit('학습용 문제', '정답')
prediction = gil.predict('테스트용 문제')


>>> Female/Male Classification_ML_Simple

읽기 >
csv 파일 읽기 :
import pandas as pd
data = pd.read_csv("asdf.csv")

표시 >
점 찍기 :
plot(data, "Height", "Weight", "Sex")
바이올린 형태 분포도 :
violinplot(data, "Height", "Sex")

나누기 >
학습용 80%, 테스트용 20%로 데이터 나누기 :
train, test = split(data, 0.8)
학습 입력 & 정답 :
train_x = train[['Height', 'FeetSize', 'Weight']]
train_y = train.Sex
테스트 입력 & 정답 :
test_x = test[['Height', 'FeetSize', 'Weight']]
test_y = test.Sex

---------------- 0323
학습 >
정답 5개 호출 :
test_x.head(5)
학습시키기 & 예측하기 :
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
gil = svm.SVC()
gil.fit(train_x, train_y)
prediction = gil.predict(test_x)

테스트 >
print('인식률 : ', metrics.accuracy_score(prediction, test_y) * 100)
gil = LogisticRegression()
gil = KNeighborsClassifier(n_neighbors = 3)


>>> plant_diary_original_simple

데이터 처리 모듈 :
import pandas as pd
테이터 시각화 모듈 :
import matplotlib.pyplot as plt
import seaborn as sns
데이터 분할 모듈 :
from sklearn.model_selection import train_test_split

from sklearn.tree import DecisionTreeRegressor
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.ensemble import RandomForestRegressor

나누기 >
train, test = split(data, 0.8)
train_x = train[['day']]
train_y = train.height
test_x = test[['day']]
test_y = test.height

학습(4가지) >
취사선택 :
gil = LinearRegression()
gil = KNeighborsRegressor(n_neighbors = 2)
gil = RandomForestRegressor(n_estimators = 28, random_state = 0)
gil = DecisionTreeRegressor(random_state = 0)LinearRegression()
이후 공통 사항 :
gil.fit(train_x, train_y)
score = gil.score(test_x, test_y)
print('score : ', format(score, '.3f'))

테스트 >
prediction = gil.predict(test_x)
print('예상 : ', prediction)
print('정답 : ', test_y)


★머신러닝의 핵심 ::
종류 이거 or 저거? : 분류
-> 비가 올까 안 올까, 더울까 추울까, 학년이 어떻게 될까
값이 얼만큼 될까? : 예측
-> 몸무게가 어떻게 될까, 내일 온도는 어떨까


머신러닝 중에서 사람의 뇌를 모방 : 딥러닝.

-------------- 0329
딥러닝 :: 시냅스(연결부위, 신경망)의 구현.
★시냅스에서 신경전달물질(단백질)이 수용체로 이동, 물질이 많이 나올 수록 전달되는 전기신호가 강해진다.
뇌는 신호가 발생 or X로만 구분.
신호 * 신경전달물질의 양 = 전기신호 -> 특정 레벨 이상일 때만 발생 판정 -> 신경세포 핵에서 전기신호 합치기
신경전달물질의 설정에 따라 전기신호가 다르게 발생. 걷는 용도, 기억하는 용도 등

---------------- 0330
신호 -> 신경전달물질 -> 물질이 수용체로 이동 -> 반대편 뉴런 자극 -> 신경전달물질을 원래 자리 회수 -> 반복

뇌 문제 :
시냅스 작동 X -> 마비, 치매 등
= 시냅스 설정 X, 신경전달물질의 양 제대로 설정 X
-> 딥러닝은 신경전달물질의 양을 바꾸는 프로그래밍.

습관 바꾸기? :
부정적 이미지 심기. 고통 주입. -> 신경전달물질의 양 변화로 기억하기.
= 뇌내 학습의 원리? : 행복은 늘리고 고통은 줄이는 방향으로.

그림책 : 입력을 주고 정답을 알려준다. -> 학습으로 신경전달물질 설정하기.
gil.fit(<그림>, '오리')


뉴런의 동작(출력) ::
신경세포가 단 하나라 가정. (w=1)
신경세포 출력 h = 입력 x * 연결강도(신경전달물질 양 : 단절/약/강) w
w=1인 뉴런 : h = 1x

★학습이란? :
신경전달물질의 양(w)을 조절하는 것.

ex)
1시간(x) 공부하면 1시간(ground truth) 게임.
그럼 4시간 공부에 몇 시간 게임할 수 있을지(prediction) w값 구하기.
h = w * x(=1)
입력과 정답이 정해졌다면 강도의 변화에 따라 오차를 찾아내(야단) 줄여나간다.

실제로는 여러 개의 입력. 입력의 수만큼 연결도 존재.
-> 각 입력마다 시냅스의 가중치를 곱한 뒤 모두 더한다. (weighted sum)

뉴런은 weighted sum이 일정 값 이상일 때만 신호 발생. (계단 함수)
= 특정 값(T) 이상이면 ON(1), 아니면 OFF(0)
신경세포 핵에서 weighted sum & 계단 함수 역할.

★시험 : 뉴런 그리기.
h = 1x
h = x1 + 2x2 + x3 + 3x4
h = 1 (if x1 + 2x2 + x3 + 3x4 > T) / 0 (otherwise)

-------------- 0405
아기가 불을 만져도 떼지 못하는 이유?
: 시냅스가 설정되지 않아서 뉴런 간에 고통 -> 회피 구성이 없음.
뉴런의 출력 = 대답.


[선형회귀]
회귀(regression) :
자연의 법칙, 현상. 그에 따른 예측이 가능.
ex) 연어, 연령과 힘의 관계

선형회귀 :
비례 관계의 회귀. 선형회귀를 모아서 비선형 예측 가능.
-> 회귀 형태가 비선형이어도 선형회귀라 할 수 있다.
ex) 집의 크기-가격의 직선 그래프
x가 어떠한 선형결합 상태인지.
y축(가격) : 뉴런이 맞춰야 할 정답, 그를 결정하는 팩터들이 많다. 학군, 혐오시설...

---------------- 0406
https://www.desmos.com/	그래프 그리기.
csv : 컴마로 구분된, 학습할 데이터가 있는 파일

ex) 연결강도 w 찾기.
인스턴스 (1,1),(2,2),(3,3) 보고 h=1x 유추해내면 정확.
만약 w=2? -> 오류. -> 수정 요청하면 학습, 맞을 때까지

경험이란? : 다양한 경우를 csv로 저장한다.
입력이 2개면 3차원 그래프. 그 이상은 X

h = wx + b
하나의 뉴런은 하나의 선형회귀를 표현. 직선이 회귀.
h : 가설, 뉴런의 대답
연결강도 w, 바이어스 b : 시냅스 연결강도의 값. 다양한 회귀 표현 가능.
-> ★w와 b를 어떻게 조절? : 뉴런이 정답을 더 잘 대답하도록 조절하기.

★★그래프 모양의 유래 설명하기.
뉴런이 맞는 대답을 하는지 아는 방법? :
h와 기존의 정답(ground truth) y의 오차를 계산.
★차이는 간격이므로 음수가 나지 않는다. -> 절대값.
-> ★오류 E = |xw - y| (x축: w, y축: E)
-> ★학습이란? : 오류(E)가 거의 없도록 w를 조절하는 것.

가설(h) : 어떤 현상을 설명, 표현한 것. 증명되지 않았지만 실험과 조절로 검증할 수 있는 것.

ex)
E = |w*1 - 1| 학습 데이터? -> 1,1
csv (1,1)(2,2)(3,3) 오류값? -> E = |w*1 - 1| + |w*2 - 2| + |w*3 - 3|
E = |w*1 - 0.5| 학습 완료? -> w=0.5
E = |w*1 - 3| 학습 완료 그래프? -> h=3x

지도학습 : 입력과 정답을 알려주면서 학습한다.


위에 나온 절대값 오류 함수는 V자 형태, 오류 줄도록 w 조절하려면?
-> 경사하강. : 기울기를 이용해 계속 경사 아래로 내려가면 에러가 없는 지점에 도달할 것.

다음 w값 좌표 : w1 = w0 - a*기울기 (a : 반영 비율)
ex) w = 4 - 1*1 or w = -2 - 1*(-1)

------------- 0412
yungcheolbyun@gmail.com
절댓값 오류 함수에서 밖으로 튕겨나갈 때의 용어? 학습률 오버슈팅.
오늘 내로.

절대값 오류 함수의 기울기로 w를 고친다.
★경사하강 수식 :
w = w - a*기울기 (★기울기는 w지점에서의 기울기.)
ex) E = |w - 1|
w=4 : w = 4 - 1*1 -> w=3
-> w=3 : w = 3 - 1*1 -> w=2
-> w=2 : w = 2 - 1*1 -> w=1

w=-2 : w = -2 - 1*(-1) -> w=-1 ~

if a(반영 비율) = 2?
w=-2 : w = -2 - 2*(-1) -> w=0
-> w=0 : w = 0 - 2*(-1) -> w=2
-> w=2 : w = 2 - 2*(1) -> w=0 무한 루프

절대값 오류? :
1. 왼쪽-오른쪽은 항상 같은 기울기 = 항상 같은 경사하강 속도.
2. w값 조절 실패 가능
3. w=1일 때 기울기 구할 수 없다.
-> 기울기만 보고 w값 짐작 불가.


제곱 오류 함수 (square error func) :
E = |xw - y|^2
E=0에 가까울 수록 에러는 더 작아진다.
ex) 0.5*1=0.5, 한편 0.5*0.5=0.25

마찬가지로 w 지점의 기울기만 구한다면 경사하강 가능.
-> ★미분 : x축 변화를 아주 작게 해 접선으로 정확한 기울기 구하기.
lim(∆w->0)∆E/∆w = ∂E/∂w
w를 아주 조금 늘렸을 때(∂w) E는 얼마나 늘어날까.(∂E)
델타 = 차이

sin을 미분하면 cos

------------ 0413
기울기 구하는 표현 : lim(∆w->0)∆E/∆w = ∂E/∂w
미분 없이도 가능, 빼는 차이가 작아야.
★개같은 그래프 주고 미분하라고 할 가능성. 기울기 0 부분 확인. 증감 속도 고려.

★에러 그래프에 제곱을 한 이유? :
에러 최저점을 찾을 때, 멀리 있는 값의 기울기는 커지고 가까운 값의 기울기는 급격히 작아져 최저점에 근접하기 쉬워진다.
+ 모든 지점에서 기울기 계산 가능.
w 지점의 기울기가 매우 크다? : 최저점에서 멀리 있다.
반대로 매우 작다? : 최저점 근처에 있다.
음식의 간 맞추기, 주차, ~

지도학습 : 입력과 정답을 알려주면서 학습한다.

평균 제곱 오류 함수(MSE) : 오차를 나타내는 척도
E = 1/3 * ((w*1 - 1)^2 + (w*2 - 2)^2 + (w*3 - 3)^2)
-> E = 1/N * N∑{i=1}(wxi - yi)^2 [N개 데이터]

https://gooopy.tistory.com/68
★에폭(epoch) :
모든 데이터를 한 번씩 학습한 횟수, 너무 많으면 오버피팅.
★배치 사이즈(batch size) :
연산 한 번에 들어가는 데이터 크기.

제곱 함수 등으로 인해 학습량 너무 많을 때 : 크게 묶어 정규화한 값들 여러 개로 판단
-> 저 둘을 적당히 조절해야 최적의 속도로 구할 수 있다.

기울기가 크다?(∆E/∆w) :
w 조절하면 E가 크게 변화 -> w 변화가 E에 미치는 영향 크다.
기울기가 작다? :
w 조절하면 E가 작게 변화 -> w 변화가 E에 미치는 영향 작다.

ex) 미치는 영향 구하기.
E = (wx - y)^2에서 데이터 (1,1) 주어질 때, w=3 지점에서 w 변화가 E에 미치는 영향?
1) 작은 차이 대입 2) w 기준 미분

https://github.com/yungbyun/myml/blob/master/n01_simple.py
구글 코랩 실행, plt 추가
★코드 fit 역할 : 에폭 횟수만큼 학습시키기.

=중간 : 26일 오후 3시쯤=

--------------- 0419
학습방법(w 업데이트) :
시냅스 연결강도(w)를 난수 초기화 (w=4)
-> w 지점에서 오류 그래프의 기울기 구하기
-> 기울기로 w 빼기 (경사하강)
w = w - a*기울기

텐서 :
무언가를 잡아당길 때 그 주위의 복잡한 변형.
경사가 큰 쪽으로 흘러간다. 계곡처럼.
-> w가 많을 수록 모든 값에 대한 복잡한 변형.
텐서플로우 내 생성값(난수), 텐서값 담는 데이터와 텐서 이용한 수식 모두 텐서로 규정.
ex) w 2개와 z축 에러 -> 경사하강 중에 깊은 구덩이로 들어가면 w값들 크게 변화

★TF로 선형 회귀 학습 :
1. x=[1] 1시간 일을 하면-
2. y=[1] 1만원을 준다.
3. w = tf.Variable(tf.random_normal([1])) 연결강도 설정
4. h = w * x 신경 세포의 출력값?
5. E = (h - y)^2 에러함수 생성
6. 경사하강으로 최적화 : train 객체(경사하강 실행), 세션 객체(train 다루는 역할)

train = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(E)
sess = tf.Session()
sess.run(tf.global_variables_initializer()) #𝑤값 초기화
err_list = []
for i in range(101):
w_val = sess.run(w)
err = sess.run(E)
print(i, 'w:', w_val, 'cost:', err)
err_list.append(err)
★sess.run(train) #이 함수의 역할? : train 호출해 경사하강 1번

7. 학습이 끝나고 테스트
8. 에러 그래프



0427 중간 끝 ---------------
텐서의 파생도 텐서.
for i in range(101):
	print(sess.run(w), sess.run(E))
	sess.run(train)
에폭 101번 = 경사하강 101번

텐서 h=wx 처음 만들 때 적용한 x값이 그대로 사용 -> 원하는 값 넣는 방법?
= 자리표시기.(placeholder)

w = tf.Variable(tf.random_normal([1]))
hypo = w * X
cost = (hypo  - Y) ** 2
~
print(sess.run(w), sess.run(cost, feed_dict={X:x_data, Y:y_data}))
~
print(sess.run(hypo, feed_dict={X:[3, 5, 7, 8, 10]}))
X에 x_data를, Y에 y_data를 삽입한다.

★텐서플로의 오류 계산 그래프 ::
𝑤의 변화가 𝐸에 미치는 영향(기울기)을 구한 후 경사하강하도록 𝑤를 조절
곱하기에서 옆의 값 = 큰 값에 미치는 영향.

--------------- 0503
논문의 모듈화. 내일 휴강.
과제 : sigmoid 그래프의 기원과 용도 조사, 미분 현황? 반장 분량

로지스틱 회귀 ::
조건 변경 - 몇 시간을 공부해도 노는 시간은 1시간, 아니면 없음.
선형 회귀(h=wx)로는 비선형적 구조를 학습할 수 없다.
-> 기존에 주어진 그래프를 적용시키기. h=1/(1+e^-wx)
★sigmoid 그래프에서의 결정경계 x값? : x가 0이 되어야.

뉴런 핵에선 이미 sigmoid의 작용을 하고 있다.
★뉴런 그림만으로 결정경계 구하기.

h가 sigmoid로 변화한 뒤 에러함수 적용 가능? -> 불가. 새 함수 필요.

--------------- 0511
train 객체의 에폭 진행과 역전파

시그모이드가 미치는 영향(미분) : sig'(x) = sig(x)*(1-sig(x))
& 시그모이드의 값은 0과 1 사이.
-> sig'(x)의 값은 매우 작다. -> ★ w에 미치는 영향도 매우 작다. (vanishing gradient)
-> 학습이 제대로 안 된다. -> 인공지능 멸망, 머신러닝의 시대.

결정경계: wx+b=0
-> ★ w, b를 업데이트(학습)한다는 의미? : 더 나은 결정경계를 찾는 것.

reduce_mean : 각종 데이터의 평균 구하기. -> 하나의 에러값 구해 수정.

--------------- 0517
스마트그리드:
태양광 등 분산전원으로 인한 과다한 전력 조절하는 방법? -> VPP 플랫폼 기술
광역 전력망의 한계 -> 마이크로그리드 플랫폼
DR 기술로 시간대 전기 사용 혜택 주려면 사용량 예측 필요.

신경 세포에 입력 추가:
입력 2개 되면 OR 판정 -> 조건 하나만 만족해도 1, 아니면 0
-> 결정경계 w1x1 + w2x2 + b = 0
OR 뿐만 아니라 NAND 등 사용. (모든 디바이스 구현 가능)